{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decomposing SST Reviews into subphrases of opposite sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data, datasets\n",
    "import torch\n",
    "import os\n",
    "from collections import Counter\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from ContextualDecomposition import CD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = data.Field(lower='preserve-case')\n",
    "answers = data.Field(sequential=False, unk_token=None) # y: floats\n",
    "train, dev, test = datasets.SST.splits(inputs, answers, fine_grained = False, train_subtrees = True)\n",
    "inputs.build_vocab(train, dev, test)\n",
    "inputs.vocab.load_vectors('glove.6B.300d')\n",
    "answers.build_vocab(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse the reviews into positive and negative review with opposing sentiment subsentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseTrees(train):\n",
    "    # Output two list: Positive reviews, Negative reviews with opposing sentiment subsentences\n",
    "    # List of tuples: (sentence, list of subsentence) where subsentences are of opposing sentiment to sentence.\n",
    "    # The subsentences are of lengths 1/3 and 2/3 of the given sentence\n",
    "    positivels = []\n",
    "    negativels = []\n",
    "    cur_sentence = []\n",
    "    for _,sub in enumerate(train):\n",
    "        # check if sub is subsentence of curr_sentence\n",
    "        if set(sub.text).issubset(set(cur_sentence)):\n",
    "            l = len(sub.text)\n",
    "            # Check if length of subtree is between 1/3 and 2/3\n",
    "            if (l <= cur_length*2/3.0) and (l >= cur_length/3.0) :\n",
    "                # get sentiment of subsentence\n",
    "                sentiment = sub.label\n",
    "                # add subsentence to corresponding list, want opposing sentiment subphrases\n",
    "                if sentiment == 'positive' and cur_sentiment == 'negative':\n",
    "                    negativels[-1][1].append(sub)\n",
    "                elif sentiment == 'negative' and cur_sentiment == 'positive':\n",
    "                    positivels[-1][1].append(sub)\n",
    "        else:\n",
    "            cur_sentiment = sub.label\n",
    "            if cur_sentiment == 'negative':\n",
    "                negativels.append((sub, []))\n",
    "            elif cur_sentiment == 'positive':\n",
    "                positivels.append((sub,[]))\n",
    "            cur_sentence = sub.text\n",
    "            cur_length = len(cur_sentence)\n",
    "\n",
    "    # remove all sentence with empty subphrase list\n",
    "    pls = []\n",
    "    nls = []\n",
    "    for e in positivels:\n",
    "        if len(e[1]): pls.append(e)\n",
    "    for e in negativels:\n",
    "        if len(e[1]): nls.append(e)\n",
    "            \n",
    "    return [pls, nls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format the lists for the CD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window(phrase, sub):\n",
    "    tups = []\n",
    "    for i in range(phrase.shape[0]):\n",
    "        if i + len(sub) > phrase.shape[0]:\n",
    "            break\n",
    "        else:\n",
    "            if np.array_equal(phrase[i:i+len(sub)], sub):\n",
    "                tups.append((i, i+len(sub)-1))\n",
    "    return tups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_indices(ls):\n",
    "    formatted_ls = []\n",
    "    for tup in ls:\n",
    "        phrase = inputs.numericalize([tup[0].text], device=-1, train=False)\n",
    "        subphrases = [inputs.numericalize([sub.text], device=-1, train=False) for sub in tup[1]]\n",
    "        np_phrase = phrase.data.numpy()\n",
    "        idx_tups = []\n",
    "        for sub in subphrases:\n",
    "            np_sub = sub.data.numpy()\n",
    "            idx_tups += rolling_window(np_phrase, np_sub)\n",
    "        formatted_ls.append((phrase, idx_tups))\n",
    "    return formatted_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pls, nls = parseTrees(train)\n",
    "pls = format_indices(pls)\n",
    "nls = format_indices(nls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: list of phrase and corresponding subphrases\n",
    "#        label of list: positive or negative phrases (with opposite sentiment subphrases)\n",
    "#        model\n",
    "def findSubphraseScores(ls, label, model):\n",
    "    output = []\n",
    "    for phrase, subIndexList in ls:\n",
    "        for start,stop in subIndexList:\n",
    "            e = CD(phrase, model, start, stop)\n",
    "            if label == 'positive': # if phrase is positive, the subphrases are negative\n",
    "                output.append(e[0])\n",
    "            elif label == 'negative': # if phrase is negative, the subphrases are positive\n",
    "                output.append(e[1])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "plsScores = findSubphraseScores(pls, 'positive', model)\n",
    "nlsScores = findSubphraseScores(nls, 'negative', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decomp_three(a, b, c, activation):\n",
    "    a_contrib = 0.5 * (activation(a + c) - activation(c) + activation(a + b + c) - activation(b + c))\n",
    "    b_contrib = 0.5 * (activation(b + c) - activation(c) + activation(a + b + c) - activation(a + c))\n",
    "    return a_contrib, b_contrib, activation(c)\n",
    "\n",
    "def decomp_tanh_two(a, b):\n",
    "    return 0.5 * (np.tanh(a) + (np.tanh(a + b) - np.tanh(b))), 0.5 * (np.tanh(b) + (np.tanh(a + b) - np.tanh(a)))\n",
    "    \n",
    "\n",
    "def makedirs(name):\n",
    "    \"\"\"helper function for python 2 and 3 to call os.makedirs()\n",
    "       avoiding an error if the directory to be created already exists\"\"\"\n",
    "\n",
    "    import os, errno\n",
    "\n",
    "    try:\n",
    "        os.makedirs(name)\n",
    "    except OSError as ex:\n",
    "        if ex.errno == errno.EEXIST and os.path.isdir(name):\n",
    "            # ignore existing directory\n",
    "            pass\n",
    "        else:\n",
    "            # a different error happened\n",
    "            raise\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = ArgumentParser(description='PyTorch/torchtext SST')\n",
    "    parser.add_argument('--epochs', type=int, default=5)\n",
    "    parser.add_argument('--batch_size', type=int, default=50)\n",
    "    parser.add_argument('--d_embed', type=int, default=300)\n",
    "    parser.add_argument('--d_proj', type=int, default=300)\n",
    "    parser.add_argument('--d_hidden', type=int, default=128)\n",
    "    parser.add_argument('--n_layers', type=int, default=1)\n",
    "    parser.add_argument('--log_every', type=int, default=1000)\n",
    "    parser.add_argument('--lr', type=float, default=.001)\n",
    "    parser.add_argument('--dev_every', type=int, default=1000)\n",
    "    parser.add_argument('--save_every', type=int, default=1000)\n",
    "    parser.add_argument('--dp_ratio', type=int, default=0.2)\n",
    "    parser.add_argument('--no-bidirectional', action='store_false', dest='birnn')\n",
    "    parser.add_argument('--preserve-case', action='store_false', dest='lower')\n",
    "    parser.add_argument('--no-projection', action='store_false', dest='projection')\n",
    "    parser.add_argument('--train_embed', action='store_false', dest='fix_emb')\n",
    "    parser.add_argument('--gpu', type=int, default=0)\n",
    "    parser.add_argument('--save_path', type=str, default='results')\n",
    "    parser.add_argument('--vector_cache', type=str, default=os.path.join(os.getcwd(), '.vector_cache/input_vectors.pt'))\n",
    "    parser.add_argument('--word_vectors', type=str, default='glove.6B.300d')\n",
    "    parser.add_argument('--resume_snapshot', type=str, default='')\n",
    "    args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08393843, 0.91606157])"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    scoreMatExp = np.exp(np.asarray(x))\n",
    "    return scoreMatExp / scoreMatExp.sum(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdb\n",
    "import torch\n",
    "import numpy as np\n",
    "from argparse import ArgumentParser\n",
    "from torchtext import data, datasets\n",
    "from scipy.special import expit as sigmoid\n",
    "import random\n",
    "\n",
    "# batch of [start, stop) with unigrams working\n",
    "def CDAuthor(batch, model, start, stop):\n",
    "    weights = model.lstm.state_dict()\n",
    "\n",
    "    # Index one = word vector (i) or hidden state (h), index two = gate\n",
    "    W_ii, W_if, W_ig, W_io = np.split(weights['weight_ih_l0'], 4, 0)\n",
    "    W_hi, W_hf, W_hg, W_ho = np.split(weights['weight_hh_l0'], 4, 0)\n",
    "    b_i, b_f, b_g, b_o = np.split(weights['bias_ih_l0'].cpu().numpy() + weights['bias_hh_l0'].cpu().numpy(), 4)\n",
    "    word_vecs = model.word_embeddings(batch)[0].data\n",
    "    T = word_vecs.size(0)\n",
    "    relevant = np.zeros((T, model.hidden_dim))\n",
    "    irrelevant = np.zeros((T, model.hidden_dim))\n",
    "    relevant_h = np.zeros((T, model.hidden_dim))\n",
    "    irrelevant_h = np.zeros((T, model.hidden_dim))\n",
    "    for i in range(T):\n",
    "        if i > 0:\n",
    "            prev_rel_h = relevant_h[i - 1]\n",
    "            prev_irrel_h = irrelevant_h[i - 1]\n",
    "        else:\n",
    "            prev_rel_h = np.zeros(model.hidden_dim)\n",
    "            prev_irrel_h = np.zeros(model.hidden_dim)\n",
    "\n",
    "        rel_i = np.dot(W_hi, prev_rel_h)\n",
    "        rel_g = np.dot(W_hg, prev_rel_h)\n",
    "        rel_f = np.dot(W_hf, prev_rel_h)\n",
    "        rel_o = np.dot(W_ho, prev_rel_h)\n",
    "        irrel_i = np.dot(W_hi, prev_irrel_h)\n",
    "        irrel_g = np.dot(W_hg, prev_irrel_h)\n",
    "        irrel_f = np.dot(W_hf, prev_irrel_h)\n",
    "        irrel_o = np.dot(W_ho, prev_irrel_h)\n",
    "\n",
    "        if i >= start and i <= stop:\n",
    "            rel_i = rel_i + np.dot(W_ii, word_vecs[i])\n",
    "            rel_g = rel_g + np.dot(W_ig, word_vecs[i])\n",
    "            rel_f = rel_f + np.dot(W_if, word_vecs[i])\n",
    "            rel_o = rel_o + np.dot(W_io, word_vecs[i])            \n",
    "        else:\n",
    "            irrel_i = irrel_i + np.dot(W_ii, word_vecs[i])\n",
    "            irrel_g = irrel_g + np.dot(W_ig, word_vecs[i])\n",
    "            irrel_f = irrel_f + np.dot(W_if, word_vecs[i])\n",
    "            irrel_o = irrel_o + np.dot(W_io, word_vecs[i])\n",
    "\n",
    "        rel_contrib_i, irrel_contrib_i, bias_contrib_i = decomp_three(rel_i, irrel_i, b_i, sigmoid)\n",
    "        rel_contrib_g, irrel_contrib_g, bias_contrib_g = decomp_three(rel_g, irrel_g, b_g, np.tanh)\n",
    "\n",
    "        relevant[i] = rel_contrib_i * (rel_contrib_g + bias_contrib_g) + bias_contrib_i * rel_contrib_g\n",
    "        irrelevant[i] = irrel_contrib_i * (rel_contrib_g + irrel_contrib_g + bias_contrib_g) + (rel_contrib_i + bias_contrib_i) * irrel_contrib_g\n",
    "\n",
    "        if i >= start and i < stop:\n",
    "            relevant[i] += bias_contrib_i * bias_contrib_g\n",
    "        else:\n",
    "            irrelevant[i] += bias_contrib_i * bias_contrib_g\n",
    "\n",
    "        if i > 0:\n",
    "            rel_contrib_f, irrel_contrib_f, bias_contrib_f = decomp_three(rel_f, irrel_f, b_f, sigmoid)\n",
    "            relevant[i] += (rel_contrib_f + bias_contrib_f) * relevant[i - 1]\n",
    "            irrelevant[i] += (rel_contrib_f + irrel_contrib_f + bias_contrib_f) * irrelevant[i - 1] + irrel_contrib_f * relevant[i - 1]\n",
    "\n",
    "        o = sigmoid(np.dot(W_io, word_vecs[i]) + np.dot(W_ho, prev_rel_h + prev_irrel_h) + b_o)\n",
    "        rel_contrib_o, irrel_contrib_o, bias_contrib_o = decomp_three(rel_o, irrel_o, b_o, sigmoid)\n",
    "        new_rel_h, new_irrel_h = decomp_tanh_two(relevant[i], irrelevant[i])\n",
    "        #relevant_h[i] = new_rel_h * (rel_contrib_o + bias_contrib_o)\n",
    "        #irrelevant_h[i] = new_rel_h * (irrel_contrib_o) + new_irrel_h * (rel_contrib_o + irrel_contrib_o + bias_contrib_o)\n",
    "        relevant_h[i] = o * new_rel_h\n",
    "        irrelevant_h[i] = o * new_irrel_h\n",
    "\n",
    "    W_out = model.hidden2label.weight.data\n",
    "    \n",
    "    # Sanity check: scores + irrel_scores should equal the LSTM's output minus model.hidden_to_label.bias\n",
    "    scores = np.dot(W_out, relevant_h[T - 1])\n",
    "    irrel_scores = np.dot(W_out, irrelevant_h[T - 1])\n",
    "\n",
    "    return scores, irrel_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph \n",
    "import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "num_bins = 30\n",
    "# the histogram of the data\n",
    "\n",
    "n, bins, patches = plt.hist(nlsScores, num_bins, normed=1, facecolor='red', alpha=0.5)\n",
    "n, bins, patches = plt.hist(plsScores, num_bins, normed=1, facecolor='green', alpha=0.5)\n",
    "# add a 'best fit' line\n",
    "y = mlab.normpdf(bins, mu, sigma)\n",
    "plt.plot(bins, y, 'r--')\n",
    "plt.xlabel('Smarts')\n",
    "plt.ylabel('Probability')\n",
    "\n",
    "# Tweak spacing to prevent clipping of ylabel\n",
    "plt.subplots_adjust(left=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.04279443431226758"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(plsScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = inputs.numericalize([[\"A\"], [\"painfully\"], [\"funny\"], [\"ode\"], [\"to\"], [\"bad\"], [\"behaviour\"], [\".\"]], device=-1, train=False)\n",
    "y = inputs.numericalize([[\"A\"], [\"bad\"], [\"behaviour\"], [\".\"]], device=-1, train=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.43579833, -0.52105784]), array([ 1.62470559, -1.72377115]))\n",
      "(array([ 2.2601066, -2.4357482]), array([-1.10472022,  1.10995205]))\n"
     ]
    }
   ],
   "source": [
    "print(CDAuthor(x,model,1,1))\n",
    "print(CDAuthor(y,model,1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main sentence: \n",
      "[u'have', u'had', u'enough', u'of', u'plucky', u'british', u'eccentrics', u'with', u'hearts', u'of', u'gold']\n",
      "Positive subsentence: \n",
      "[u'with', u'hearts', u'of', u'gold']\n",
      "Negative subsentence: \n",
      "[u'enough', u'of', u'plucky', u'british', u'eccentrics']\n"
     ]
    }
   ],
   "source": [
    "print(\"Main sentence: \")\n",
    "print(sentencels[3][0].text)\n",
    "print(\"Positive subsentence: \")\n",
    "print(sentencels[3][1][0][0].text)\n",
    "print (\"Negative subsentence: \")\n",
    "print(sentencels[3][1][1][0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main sentence: \n",
      "[u'in', u'this', u'case', u'zero', u'.']\n",
      "Positive subsentence: \n",
      "[u'case', u'zero', u'.']\n",
      "Positive subsentence: \n",
      "[u'zero', u'.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Main sentence: \")\n",
    "print(sentencels[-1][0].text)\n",
    "print(\"Positive subsentence: \")\n",
    "print(sentencels[-1][1][1][0].text)\n",
    "print (\"Positive subsentence: \")\n",
    "print(sentencels[-1][1][1][1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main sentence: \n",
      "[u'next', u'pretty', u'good', u'thing']\n",
      "Positive subsentence: \n",
      "[u'good', u'thing']\n"
     ]
    }
   ],
   "source": [
    "print(\"Main sentence: \")\n",
    "print(sentencels[30][0].text)\n",
    "print(\"Positive subsentence: \")\n",
    "print(sentencels[30][1][0][0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
