{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdb\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from argparse import ArgumentParser\n",
    "from torchtext import data, datasets\n",
    "from scipy.stats.stats import pearsonr\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "\n",
    "def load_model(file):\n",
    "    model = torch.load(file, map_location=lambda storage, loc: storage)\n",
    "    return model\n",
    "\n",
    "def linearize(a, b, c, activation_fn):\n",
    "    a_contrib = 0.5 * (activation_fn(a + b + c)  + activation_fn(a + c) - activation_fn(b + c) - activation_fn(c))\n",
    "    b_contrib = 0.5 * (activation_fn(a + b + c)  - activation_fn(a + c) + activation_fn(b + c) - activation_fn(c))\n",
    "    return a_contrib, b_contrib, activation_fn(c)\n",
    "\n",
    "def linearize_tanh(a, b):\n",
    "    a_contrib = 0.5 * (np.tanh(a) + np.tanh(a + b) - np.tanh(b)) \n",
    "    b_contrib = 0.5 * (np.tanh(b) + np.tanh(a + b) - np.tanh(a))\n",
    "    return a_contrib, b_contrib\n",
    "\n",
    "def CD(batch, model, start, stop):\n",
    "    weights = model.lstm.state_dict()\n",
    "    \n",
    "    W_ii, W_if, W_ig, W_io = np.split(weights['weight_ih_l0'], 4, 0)\n",
    "    W_hi, W_hf, W_hg, W_ho = np.split(weights['weight_hh_l0'], 4, 0)\n",
    "    W_out = model.hidden2label.weight.data\n",
    "    b_i, b_f, b_g, b_o = np.split(weights['bias_ih_l0'].cpu().numpy() + weights['bias_hh_l0'].cpu().numpy(), 4)\n",
    "\n",
    "    # The second axis is garbage, we remove it. Resulting matrix is of size (#words) x (length of glove vector, 300)\n",
    "    word_vecs = model.word_embeddings(batch.text)[:,0].data\n",
    "    #word_vecs = model.word_embeddings(review)[:,0].data\n",
    "    \n",
    "    L = word_vecs.size(0)\n",
    "    phrase = np.zeros((L, model.hidden_dim))  #phrase contribution\n",
    "    rest = np.zeros((L, model.hidden_dim))    #rest of the contribution\n",
    "    phrase_h = np.zeros((L, model.hidden_dim))\n",
    "    rest_h = np.zeros((L, model.hidden_dim))\n",
    "    \n",
    "    #iterate through word_vecs\n",
    "    for i in range(L):\n",
    "        if i == 0:\n",
    "            #there is no prev\n",
    "            prev_phrase_h = np.zeros(model.hidden_dim)\n",
    "            prev_rest_h = np.zeros(model.hidden_dim)\n",
    "        else:\n",
    "            prev_phrase_h = phrase_h[i-1]\n",
    "            prev_rest_h = rest_h[i-1]\n",
    "            \n",
    "        #calculating o, f, i, g    \n",
    "        phrase_o = np.dot(W_ho, prev_phrase_h)\n",
    "        phrase_f = np.dot(W_hf, prev_phrase_h)\n",
    "        phrase_i = np.dot(W_hi, prev_phrase_h)\n",
    "        phrase_g = np.dot(W_hg, prev_phrase_h)\n",
    "        \n",
    "        rest_o = np.dot(W_ho, prev_rest_h)\n",
    "        rest_f = np.dot(W_hf, prev_rest_h)\n",
    "        rest_i = np.dot(W_hi, prev_rest_h)\n",
    "        rest_g = np.dot(W_hg, prev_rest_h)\n",
    "\n",
    "        #only modify for range [start, stop]\n",
    "        if (start <= i) and (i <= stop):\n",
    "            phrase_o = phrase_o + np.dot(W_io, word_vecs[i])\n",
    "            phrase_f = phrase_f + np.dot(W_if, word_vecs[i])\n",
    "            phrase_i = phrase_i + np.dot(W_ii, word_vecs[i])\n",
    "            phrase_g = phrase_g + np.dot(W_ig, word_vecs[i])\n",
    "        else:\n",
    "            rest_o = rest_o + np.dot(W_io, word_vecs[i])\n",
    "            rest_f = rest_f + np.dot(W_if, word_vecs[i])\n",
    "            rest_i = rest_i + np.dot(W_ii, word_vecs[i])\n",
    "            rest_g = rest_g + np.dot(W_ig, word_vecs[i])\n",
    "        \n",
    "        #calculate contributions to i, g\n",
    "        phrase_contrib_i, rest_contrib_i, bias_contrib_i = linearize(phrase_i, rest_i, b_i, sigmoid)\n",
    "        phrase_contrib_g, rest_contrib_g, bias_contrib_g = linearize(phrase_g, rest_g, b_g, np.tanh)\n",
    "\n",
    "        phrase[i] = phrase_contrib_i * (phrase_contrib_g + bias_contrib_g) + bias_contrib_i * phrase_contrib_g\n",
    "        rest[i] = rest_contrib_i * (phrase_contrib_g + rest_contrib_g + bias_contrib_g) + (phrase_contrib_i + bias_contrib_i) * rest_contrib_g\n",
    "\n",
    "        #add bias for range [start,stop)\n",
    "        if i >= start and i < stop:\n",
    "            phrase[i] += bias_contrib_i * bias_contrib_g\n",
    "        else:\n",
    "            rest[i] += bias_contrib_i * bias_contrib_g\n",
    "        \n",
    "        #When there's a prev, calculate contributions\n",
    "        if i > 0:\n",
    "            phrase_contrib_f, rest_contrib_f, bias_contrib_f = linearize(phrase_f, rest_f, b_f, sigmoid)\n",
    "            phrase[i] += (phrase_contrib_f + bias_contrib_f) * phrase[i-1]\n",
    "            rest[i] += (phrase_contrib_f + rest_contrib_f + bias_contrib_f) * rest[i-1] + rest_contrib_f * phrase[i-1]\n",
    "\n",
    "        o = sigmoid(np.dot(W_io, word_vecs[i]) + np.dot(W_ho, prev_phrase_h + prev_rest_h) + b_o)\n",
    "        phrase_contrib_o, rest_contrib_o, bias_contrib_o = linearize(phrase_o, rest_o, b_o, sigmoid)\n",
    "        new_phrase_h, new_rest_h = linearize_tanh(phrase[i], rest[i])\n",
    "        phrase_h[i] = o * new_phrase_h\n",
    "        rest_h[i] = o * new_rest_h\n",
    "    \n",
    "    #calculating final scores\n",
    "    phrase_scores = np.dot(W_out, phrase_h[L-1])\n",
    "    rest_scores = np.dot(W_out, rest_h[L-1])\n",
    "\n",
    "    return phrase_scores, rest_scores, phrase_h[L-1], rest_h[L-1]\n",
    "\n",
    "def load_model(file):\n",
    "    model = torch.load(file, map_location=lambda storage, loc: storage)\n",
    "    return model\n",
    "\n",
    "def get_batches(batch_nums, train_iterator, dev_iterator, dset='train'):\n",
    "    print('Getting batches.')\n",
    "    # pick data_iterator\n",
    "    if dset=='train':\n",
    "        data_iterator = train_iterator\n",
    "    elif dset=='dev':\n",
    "        data_iterator = dev_iterator\n",
    "    #get batches\n",
    "    num = 0\n",
    "    batches = {}\n",
    "    data_iterator.init_epoch() \n",
    "    for batch_idx, batch in enumerate(data_iterator):\n",
    "        if batch_idx == batch_nums[num]:\n",
    "            batches[batch_idx] = batch\n",
    "            num +=1 \n",
    "\n",
    "        if num == max(batch_nums):\n",
    "            break\n",
    "        elif num == len(batch_nums):\n",
    "            print('found them all')\n",
    "            break\n",
    "    return batches\n",
    "\n",
    "def get_sst():    \n",
    "    inputs = data.Field(lower='preserve-case')\n",
    "    answers = data.Field(sequential=False, unk_token=None)\n",
    "\n",
    "    # build with subtrees so inputs are right\n",
    "    train_s, dev_s, test_s = datasets.SST.splits(inputs, answers, fine_grained = False, train_subtrees = True,\n",
    "                                           filter_pred=lambda ex: ex.label != 'neutral')\n",
    "    inputs.build_vocab(train_s, dev_s, test_s)\n",
    "    answers.build_vocab(train_s)\n",
    "    \n",
    "    # rebuild without subtrees to get longer sentences\n",
    "    train, dev, test = datasets.SST.splits(inputs, answers, fine_grained = False, train_subtrees = False,\n",
    "                                       filter_pred=lambda ex: ex.label != 'neutral')\n",
    "    \n",
    "    train_iter, dev_iter, test_iter = data.BucketIterator.splits(\n",
    "            (train, dev, test), batch_size=1, device=-1)\n",
    "\n",
    "    return inputs, answers, train_iter, dev_iter\n",
    "\n",
    "def preprocessingBOW(batch,vocab,istrain):\n",
    "    tensor = torch.FloatTensor(len(vocab),len(batch))\n",
    "    tensor.zero_()\n",
    "    \n",
    "    for i in xrange(len(batch)):\n",
    "        # update frequency\n",
    "        c = Counter()\n",
    "        c.update(batch[i])\n",
    "        \n",
    "        localtensor = torch.FloatTensor(len(vocab))\n",
    "        localtensor.zero_()\n",
    "        \n",
    "        localtensor[c.keys()] = torch.FloatTensor(c.values())\n",
    "        tensor[:,i] = localtensor\n",
    "    \n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs, answers, train_iterator, dev_iterator = get_sst()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get batches for train and dev set\n",
    "n_dev = 872 \n",
    "batch_num_dev = list(range(n_dev+1))\n",
    "batches_dev = get_batches(batch_num_dev, train_iterator, dev_iterator, dset='dev')\n",
    "n_train = 11000\n",
    "batch_num_train = list(range((n_train+1)))\n",
    "batches_train = get_batches(batch_num_train, train_iterator, dev_iterator, dset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_train, len(batches_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load LSTM model and logistic regression weights\n",
    "model_file = \"model.pt\"\n",
    "LR_weights_file = \"LR_weights.npy\"\n",
    "\n",
    "model = load_model(model_file)\n",
    "LR_weights = np.load(LR_weights_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputsBOW = data.Field(lower='preserve-case', tensor_type=torch.FloatTensor, postprocessing=preprocessingBOW)\n",
    "\n",
    "# No tokenization applied because the data is not seq\n",
    "# unk_token=None: ignore out of vocabulary tokens, since these are grades\n",
    "answersBOW = data.Field(sequential=False, unk_token=None)\n",
    "\n",
    "# fine_grained=False - use the following grade mapping { 0,1 -> negativ; 2 -> neutral; 3,4 -> positive }\n",
    "# filter=... - remove the neutral class to reduce the problem to binary classification\n",
    "# train_subtrees=False - Use only complete review instead of also using subsentences (subtrees)\n",
    "trainBOW, devBOW, testBOW = datasets.SST.splits(inputsBOW, answersBOW, fine_grained = False, train_subtrees = True,\n",
    "                                       filter_pred=lambda ex: ex.label != 'neutral')\n",
    "# build the initial vocabulary from the SST dataset\n",
    "inputsBOW.build_vocab(trainBOW, devBOW, testBOW)\n",
    "\n",
    "# build the vocab for the labels (only consists of 'positive','negative')\n",
    "answersBOW.build_vocab(trainBOW)\n",
    "\n",
    "device=-1 #-1 for CPU and integer otherwise\n",
    "vocab_size = len(inputsBOW.vocab)\n",
    "\n",
    "trainBOW_iter, devBOW_iter, testBOW_iter = data.BucketIterator.splits(\n",
    "        (trainBOW, devBOW, testBOW), repeat=False, batch_size=1, device=device)\n",
    "\n",
    "batchesBOW = get_batches(batch_num_dev, trainBOW_iter, devBOW_iter, dset=\"dev\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_coeff = {}\n",
    "text = batchesBOW[100].text.data\n",
    "#determine logistic regression coefficient for each word in dev set\n",
    "for i in range(n_dev):\n",
    "    if (i % (100) == 0):\n",
    "        print(\"%d out of %d\" % (i, n_dev))\n",
    "    text = batches_dev[i].text.data[:, 0]\n",
    "    words = [inputs.vocab.itos[idx] for idx in text]\n",
    "    L = len(words)\n",
    "    LR_coeff[i] = np.zeros(L)\n",
    "    for j in range(L):\n",
    "        one_hot_idx = inputsBOW.vocab.stoi[words[j]]\n",
    "        word_vec = np.zeros(vocab_size)\n",
    "        word_vec[one_hot_idx] = 1\n",
    "        coeff = np.dot(word_vec,LR_weights.T)\n",
    "        LR_coeff[i][j] = coeff[0] - coeff[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CD_scores = {}\n",
    "#determine CD score for each word in dev set\n",
    "for i in range(n_dev):\n",
    "    if (i % (100) == 0):\n",
    "        print(\"%d out of %d\" % (i, n_dev))\n",
    "    batch = batches_dev[i]\n",
    "    word_vecs = model.word_embeddings(batch.text)[:,0].data\n",
    "    L = word_vecs.size(0)\n",
    "    CD_scores[i] = np.zeros(L)\n",
    "    for j in range(L):\n",
    "        phrase_score, _, _, _ = CD(batch, model, j, j)\n",
    "        CD_scores[i][j] = phrase_score[0] - phrase_score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_LR = np.array([])\n",
    "y_CD = np.array([])\n",
    "#generate numpy arrays for LR coefficients and CD score, \n",
    "#such that each point x, y correspond to a word in dev set\n",
    "for i in range(n_dev):\n",
    "    for j in range(len(CD_scores[i])):\n",
    "        x_LR = np.append(x_LR, LR_coeff[i][j])\n",
    "        y_CD = np.append(y_CD, CD_scores[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate pearson correlation coefficient r\n",
    "(r, p_value) = pearsonr(x_LR, y_CD)\n",
    "\n",
    "#calculating least squares regression line\n",
    "A = np.vstack([x_LR, np.ones(len(x_LR))]).T\n",
    "m, c = np.linalg.lstsq(A, y_CD)[0]\n",
    "\n",
    "#plotting CD scores vs LR coeffcients \n",
    "fig1 = plt.figure(1)\n",
    "plt.scatter(x_LR, y_CD, color='g', s=0.2)\n",
    "#also plot best fit line\n",
    "plt.plot(x_LR, m*x_LR + c, linewidth=1)\n",
    "plt.xlabel(\"Logistic Regression Coeffcients\")\n",
    "plt.ylabel(\"Contextual Decomposition scores\")\n",
    "fig1.savefig(\"CDvsLR_unigrams.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pearson correlation for SST:\", r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CD_embeddings_train = {}\n",
    "#calculate average embedding in train data set, for every unigram and bigram in train set \n",
    "for i in range(n_train):\n",
    "    if (i % (100) == 0):\n",
    "        print(\"%d out of %d\" % (i, n_train))\n",
    "    batch = batches_train[i]\n",
    "    text = batch.text.data[:, 0]\n",
    "    words = [inputs.vocab.itos[i] for i in text]\n",
    "    word_vecs = model.word_embeddings(batch.text)[:,0].data\n",
    "    L = len(words)\n",
    "    for j in range(L):\n",
    "        #unigrams\n",
    "        current = words[j]\n",
    "        _, _, CD_embed, _ = CD(batch, model, j, j)\n",
    "        if current in CD_embeddings_train:\n",
    "            k = CD_embeddings_train[current][0] + 1\n",
    "            CD_embeddings_train[current] = [k, CD_embeddings_train[current][1] * float(k-1)/k + CD_embed/float(k)]\n",
    "        else:\n",
    "            CD_embeddings_train[current] = [1, CD_embed]\n",
    "        #bigrams\n",
    "        if j < (L-1):\n",
    "            current = words[j] + \" \" + words[j+1]\n",
    "            _, _, CD_embed, _ = CD(batch, model, j, j+1)\n",
    "            if current in CD_embeddings_train:\n",
    "                k = CD_embeddings_train[current][0] + 1\n",
    "                CD_embeddings_train[current] = [k, CD_embeddings_train[current][1] * float(k-1)/k + CD_embed/float(k)]\n",
    "            else:\n",
    "                CD_embeddings_train[current] = [1, CD_embed]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "CD_embeddings = copy.deepcopy(CD_embeddings_train)\n",
    "\n",
    "for i in range(n_dev):\n",
    "    if (i % (100) == 0):\n",
    "        print(\"%d out of %d\" % (i, n_dev))\n",
    "    batch = batches_dev[i]\n",
    "    text = batch.text.data[:, 0]\n",
    "    words = [inputs.vocab.itos[i] for i in text]\n",
    "    word_vecs = model.word_embeddings(batch.text)[:,0].data\n",
    "    L = len(words)\n",
    "    for j in range(L):\n",
    "        #unigrams\n",
    "        current = words[j]\n",
    "        _, _, CD_embed, _ = CD(batch, model, j, j)\n",
    "        if current in CD_embeddings:\n",
    "            k = CD_embeddings[current][0] + 1\n",
    "            CD_embeddings[current] = [k, CD_embeddings[current][1] * float(k-1)/k + CD_embed/float(k)]\n",
    "        else:\n",
    "            CD_embeddings[current] = [1, CD_embed]\n",
    "        #bigrams\n",
    "        if j < (L-1):\n",
    "            current = words[j] + \" \" + words[j+1]\n",
    "            _, _, CD_embed, _ = CD(batch, model, j, j+1)\n",
    "            if current in CD_embeddings:\n",
    "                k = CD_embeddings[current][0] + 1\n",
    "                CD_embeddings[current] = [k, CD_embeddings[current][1] * float(k-1)/k + CD_embed/float(k)]\n",
    "            else:\n",
    "                CD_embeddings[current] = [1, CD_embed]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "#nearest neighbors model\n",
    "nearestNeighbors_model = NearestNeighbors(n_neighbors=20, metric='cosine')\n",
    "freq_embeddings = CD_embeddings.values()\n",
    "k_embeddings = len(freq_embeddings[0][1])\n",
    "n_embeddings = len(freq_embeddings)\n",
    "\n",
    "embeddings = np.zeros((n_embeddings, k_embeddings))\n",
    "ngram_embeddings = []\n",
    "\n",
    "for ngram, embedding in CD_embeddings.iteritems():\n",
    "    ngram_embeddings.append([ngram, embedding[1]])\n",
    "for i in range(n_embeddings):\n",
    "    embeddings[i] = freq_embeddings[i][1]\n",
    "#fit knn model to embeddings\n",
    "nearestNeighbors_model = nearestNeighbors_model.fit(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate nearest neighbours for selected phrases\n",
    "selected_phrases = ['bad', 'not bad', 'not funny', 'very funny', 'entertaining'] \n",
    "selected_embeddings = np.zeros((len(selected_phrases), k_embeddings))\n",
    "for i in range(len(selected_phrases)):\n",
    "    selected_embeddings[i] = CD_embeddings[selected_phrases[i]][1]\n",
    "    \n",
    "selected_nearestNeighbours = nearestNeighbors_model.kneighbors(selected_embeddings, return_distance=False)\n",
    "print(selected_nearestNeighbours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(selected_nearestNeighbours)):\n",
    "    print(\"Next:\")\n",
    "    for j in range(len(selected_nearestNeighbours[i])):\n",
    "        print(ngram_embeddings[selected_nearestNeighbours[i][j]][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
