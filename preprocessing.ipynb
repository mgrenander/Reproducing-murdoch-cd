{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from torchtext import data, datasets\n",
    "import torch\n",
    "import os\n",
    "from collections import Counter\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing SST into Glove Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preserves case of words\n",
    "inputs = data.Field(lower='preserve-case')\n",
    "\n",
    "# No tokenization applied because the data is not seq\n",
    "# unk_token=None: ignore out of vocabulary tokens, since these are grades\n",
    "answers = data.Field(sequential=False, unk_token=None) # y: floats\n",
    "\n",
    "# fine_grained=False - use the following grade mapping { 0,1 -> negativ; 2 -> neutral; 3,4 -> positive }\n",
    "# filter=... - remove the neutral class to reduce the problem to binary classification\n",
    "# train_subtrees=False - Use only complete review instead of also using subsentences (subtrees)\n",
    "train, dev, test = datasets.SST.splits(inputs, answers, fine_grained = False, train_subtrees = True,\n",
    "                                       filter_pred=lambda ex: ex.label != 'neutral')\n",
    "# build the initial vocabulary from the SST dataset\n",
    "inputs.build_vocab(train, dev, test)\n",
    "\n",
    "# then enhance it with the pre-trained glove model \n",
    "inputs.vocab.load_vectors('glove.6B.300d')\n",
    "\n",
    "# build the vocab for the labels (only consists of 'positive','negative')\n",
    "answers.build_vocab(train)\n",
    "\n",
    "# You can use these iterators to train/test/validate the network :)\n",
    "train_iter, dev_iter, test_iter = data.BucketIterator.splits(\n",
    "        (train, dev, test), batch_size=100, device=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bag of Words Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessingBOW(batch,vocab,istrain):\n",
    "    tensor = torch.FloatTensor(len(vocab),len(batch))\n",
    "    tensor.zero_()\n",
    "    \n",
    "    for i in xrange(len(batch)):\n",
    "        # update frequency\n",
    "        c = Counter()\n",
    "        c.update(batch[i])\n",
    "        \n",
    "        localtensor = torch.FloatTensor(len(vocab))\n",
    "        localtensor.zero_()\n",
    "        \n",
    "        localtensor[c.keys()] = torch.FloatTensor(c.values())\n",
    "        tensor[:,i] = localtensor\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "inputsBOW = data.Field(lower='preserve-case', tensor_type=torch.FloatTensor, postprocessing=preprocessingBOW)\n",
    "\n",
    "# No tokenization applied because the data is not seq\n",
    "# unk_token=None: ignore out of vocabulary tokens, since these are grades\n",
    "answersBOW = data.Field(sequential=False, unk_token=None)\n",
    "\n",
    "# fine_grained=False - use the following grade mapping { 0,1 -> negativ; 2 -> neutral; 3,4 -> positive }\n",
    "# filter=... - remove the neutral class to reduce the problem to binary classification\n",
    "# train_subtrees=False - Use only complete review instead of also using subsentences (subtrees)\n",
    "trainBOW, devBOW, testBOW = datasets.SST.splits(inputsBOW, answersBOW, fine_grained = False, train_subtrees = True,\n",
    "                                       filter_pred=lambda ex: ex.label != 'neutral')\n",
    "# build the initial vocabulary from the SST dataset\n",
    "inputsBOW.build_vocab(trainBOW, devBOW, testBOW)\n",
    "\n",
    "# build the vocab for the labels (only consists of 'positive','negative')\n",
    "answersBOW.build_vocab(trainBOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module): \n",
    "    \n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)\n",
    "        \n",
    "    def forward(self, bow_vector):\n",
    "        # Pass the input through the linear layer,\n",
    "        # then pass that through log_softmax.\n",
    "        # Many non-linearities and other functions are in torch.nn.functional\n",
    "        return F.log_softmax(self.linear(bow_vector), dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionWithoutSoftmax(nn.Module): \n",
    "    \n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        super(LogisticRegressionWithoutSoftmax, self).__init__()\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)\n",
    "        \n",
    "    def forward(self, bow_vector):\n",
    "        # Pass the input through the linear layer,\n",
    "        # then pass that through log_softmax.\n",
    "        # Many non-linearities and other functions are in torch.nn.functional\n",
    "        return self.linear(bow_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 2\n",
    "vocab_size = len(inputsBOW.vocab)\n",
    "model = LogisticRegressionWithoutSoftmax(num_labels, vocab_size)\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "device=-1 #-1 for CPU and integer otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch #0 [....................]\n",
      "epoch #1 [....................]\n",
      "epoch #2 [....................]\n",
      "epoch #3 [....................]\n",
      "epoch #4 [....................]\n"
     ]
    }
   ],
   "source": [
    "train_iter, dev_iter, test_iter = data.BucketIterator.splits(\n",
    "        (trainBOW, devBOW, testBOW), repeat=False, batch_size=100, device=device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('epoch #%s [' % epoch,end='.')\n",
    "    i=0\n",
    "    \n",
    "    for _,batch in enumerate(train_iter):\n",
    "        # Clear gradient before each new instance\n",
    "        model.zero_grad()\n",
    "        log_probs = model(batch.text)\n",
    "\n",
    "        # Compute the loss and gradients and update the parameters by opt.step()\n",
    "        loss = loss_fn(log_probs, batch.label)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        i += 1\n",
    "        if i % 50 == 0:\n",
    "            print('.',end='')\n",
    "    print(']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'LogisticRegressionModel.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "dev_loss = 0\n",
    "for idx, dev_batch in enumerate(dev_iter):\n",
    "    labels = dev_batch.label\n",
    "    prediction = model(dev_batch.text)\n",
    "    correct += (torch.max(prediction, 1)[1].view(dev_batch.label.size()).data == dev_batch.label.data).sum()\n",
    "    dev_loss = loss_fn(prediction, dev_batch.label)\n",
    "dev_acc = 100. * correct / len(devBOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.6513761468\n"
     ]
    }
   ],
   "source": [
    "print(dev_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
