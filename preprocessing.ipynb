{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data, datasets\n",
    "import torch\n",
    "import os\n",
    "from collections import Counter\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing SST into Glove Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preserves case of words\n",
    "inputs = data.Field(lower='preserve-case')\n",
    "\n",
    "# No tokenization applied because the data is not seq\n",
    "# unk_token=None: ignore out of vocabulary tokens, since these are grades\n",
    "answers = data.Field(sequential=False, unk_token=None) # y: floats\n",
    "\n",
    "# fine_grained=False - use the following grade mapping { 0,1 -> negativ; 2 -> neutral; 3,4 -> positive }\n",
    "# filter=... - remove the neutral class to reduce the problem to binary classification\n",
    "# train_subtrees=False - Use only complete review instead of also using subsentences (subtrees)\n",
    "train, dev, test = datasets.SST.splits(inputs, answers, fine_grained = False, train_subtrees = True,\n",
    "                                       filter_pred=lambda ex: ex.label != 'neutral')\n",
    "# build the initial vocabulary from the SST dataset\n",
    "inputs.build_vocab(train, dev, test)\n",
    "\n",
    "# then enhance it with the pre-trained glove model \n",
    "inputs.vocab.load_vectors('glove.6B.300d')\n",
    "\n",
    "# build the vocab for the labels (only consists of 'positive','negative')\n",
    "answers.build_vocab(train)\n",
    "\n",
    "# You can use these iterators to train/test/validate the network :)\n",
    "train_iter, dev_iter, test_iter = data.BucketIterator.splits(\n",
    "        (train, dev, test), batch_size=100, device=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bag of Words Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessingBOW(batch,vocab,istrain):\n",
    "    tensor = torch.FloatTensor(len(vocab),len(batch))\n",
    "    tensor.zero_()\n",
    "    \n",
    "    for i in xrange(len(batch)):\n",
    "        # update frequency\n",
    "        c = Counter()\n",
    "        c.update(batch[i])\n",
    "        \n",
    "        localtensor = torch.FloatTensor(len(vocab))\n",
    "        localtensor.zero_()\n",
    "        \n",
    "        localtensor[c.keys()] = torch.FloatTensor(c.values())\n",
    "        tensor[:,i] = localtensor\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "inputsBOW = data.Field(lower='preserve-case', tensor_type=torch.FloatTensor, postprocessing=preprocessingBOW)\n",
    "\n",
    "# No tokenization applied because the data is not seq\n",
    "# unk_token=None: ignore out of vocabulary tokens, since these are grades\n",
    "answersBOW = data.Field(sequential=False, unk_token=None)\n",
    "\n",
    "# fine_grained=False - use the following grade mapping { 0,1 -> negativ; 2 -> neutral; 3,4 -> positive }\n",
    "# filter=... - remove the neutral class to reduce the problem to binary classification\n",
    "# train_subtrees=False - Use only complete review instead of also using subsentences (subtrees)\n",
    "trainBOW, devBOW, testBOW = datasets.SST.splits(inputsBOW, answersBOW, fine_grained = False, train_subtrees = True,\n",
    "                                       filter_pred=lambda ex: ex.label != 'neutral')\n",
    "# build the initial vocabulary from the SST dataset\n",
    "inputsBOW.build_vocab(trainBOW, devBOW, testBOW)\n",
    "\n",
    "# build the vocab for the labels (only consists of 'positive','negative')\n",
    "answersBOW.build_vocab(trainBOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module): \n",
    "    \n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)\n",
    "        \n",
    "    def forward(self, bow_vector):\n",
    "        # Pass the input through the linear layer,\n",
    "        # then pass that through log_softmax.\n",
    "        # Many non-linearities and other functions are in torch.nn.functional\n",
    "        return F.log_softmax(self.linear(bow_vector), dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 2\n",
    "vocab_size = len(inputsBOW.vocab)\n",
    "\n",
    "model = LogisticRegression(num_labels, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, dev_iter, test_iter = data.BucketIterator.splits(\n",
    "        (trainBOW, devBOW, testBOW), batch_size=10, device=-1)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(),lr=0.0001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for _,batch in enumerate(train_iter):\n",
    "        # Clear gradient before each new instance\n",
    "        model.zero_grad()\n",
    "        log_probs = model(batch.text)\n",
    "\n",
    "        # Compute the loss and gradients and update the parameters by opt.step()\n",
    "        loss = loss_fn(log_probs, batch.label)\n",
    "        loss.backward()\n",
    "        opt.step() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Step 5: Validate\n",
    "    logloss = 0\n",
    "    for _, batch in enumerate(dev_iter):\n",
    "        log_probs = model(batch.text.data)\n",
    "        logloss += loss_fn(log_probs,batch.label.data)\n",
    "    print(\"Test log loss: \" % )\n",
    "    \n",
    "    \n",
    "# Validate\n",
    "for _, batch in enumerate(dev_iter):\n",
    "    log_probs = model(batch.text)\n",
    "    loss = loss_fn()\n",
    "    print(log_probs)\n",
    "    \n",
    "# calculate accuracy on validation set\n",
    "n_dev_correct, dev_loss = 0, 0\n",
    "for dev_batch_idx, dev_batch in enumerate(dev_iter):\n",
    "     answer = model(dev_batch)\n",
    "     n_dev_correct += (torch.max(answer, 1)[1].view(dev_batch.label.size()).data == dev_batch.label.data).sum()\n",
    "     dev_loss = criterion(answer, dev_batch.label)\n",
    "dev_acc = 100. * n_dev_correct / len(dev)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    0     0     0  ...      0     0     0\n",
       "[torch.LongTensor of size 1x18844]"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.text.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
