{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from torchtext import data, datasets\n",
    "import torch\n",
    "import os\n",
    "from collections import Counter\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing SST into Glove Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ckerriou/anaconda2/lib/python2.7/site-packages/nltk/tree.py:623: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  elif token == close_b:\n",
      "/home/ckerriou/anaconda2/lib/python2.7/site-packages/nltk/tree.py:616: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  if token[0] == open_b:\n"
     ]
    }
   ],
   "source": [
    "# preserves case of words\n",
    "inputs = data.Field(lower='preserve-case')\n",
    "\n",
    "# No tokenization applied because the data is not seq\n",
    "# unk_token=None: ignore out of vocabulary tokens, since these are grades\n",
    "answers = data.Field(sequential=False, unk_token=None) # y: floats\n",
    "\n",
    "# fine_grained=False - use the following grade mapping { 0,1 -> negativ; 2 -> neutral; 3,4 -> positive }\n",
    "# filter=... - remove the neutral class to reduce the problem to binary classification\n",
    "# train_subtrees=False - Use only complete review instead of also using subsentences (subtrees)\n",
    "train, dev, test = datasets.SST.splits(inputs, answers, fine_grained = False, train_subtrees = True,\n",
    "                                       filter_pred=lambda ex: ex.label != 'neutral')\n",
    "# build the initial vocabulary from the SST dataset\n",
    "inputs.build_vocab(train, dev, test)\n",
    "\n",
    "# then enhance it with the pre-trained glove model \n",
    "inputs.vocab.load_vectors('glove.6B.300d')\n",
    "\n",
    "# build the vocab for the labels (only consists of 'positive','negative')\n",
    "answers.build_vocab(train)\n",
    "\n",
    "# You can use these iterators to train/test/validate the network :)\n",
    "train_iter, dev_iter, test_iter = data.BucketIterator.splits(\n",
    "        (train, dev, test), batch_size=100, device=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bag of Words Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessingBOW(batch,vocab,istrain):\n",
    "    tensor = torch.FloatTensor(len(vocab),len(batch))\n",
    "    tensor.zero_()\n",
    "    \n",
    "    for i in xrange(len(batch)):\n",
    "        # update frequency\n",
    "        c = Counter()\n",
    "        c.update(batch[i])\n",
    "        \n",
    "        localtensor = torch.FloatTensor(len(vocab))\n",
    "        localtensor.zero_()\n",
    "        \n",
    "        localtensor[c.keys()] = torch.FloatTensor(c.values())\n",
    "        tensor[:,i] = localtensor\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "inputsBOW = data.Field(lower='preserve-case', tensor_type=torch.FloatTensor, postprocessing=preprocessingBOW)\n",
    "\n",
    "# No tokenization applied because the data is not seq\n",
    "# unk_token=None: ignore out of vocabulary tokens, since these are grades\n",
    "answersBOW = data.Field(sequential=False, unk_token=None)\n",
    "\n",
    "# fine_grained=False - use the following grade mapping { 0,1 -> negativ; 2 -> neutral; 3,4 -> positive }\n",
    "# filter=... - remove the neutral class to reduce the problem to binary classification\n",
    "# train_subtrees=False - Use only complete review instead of also using subsentences (subtrees)\n",
    "trainBOW, devBOW, testBOW = datasets.SST.splits(inputsBOW, answersBOW, fine_grained = False, train_subtrees = True,\n",
    "                                       filter_pred=lambda ex: ex.label != 'neutral')\n",
    "# build the initial vocabulary from the SST dataset\n",
    "inputsBOW.build_vocab(trainBOW, devBOW, testBOW)\n",
    "\n",
    "# build the vocab for the labels (only consists of 'positive','negative')\n",
    "answersBOW.build_vocab(trainBOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module): \n",
    "    \n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)\n",
    "        \n",
    "    def forward(self, bow_vector):\n",
    "        # Pass the input through the linear layer,\n",
    "        # then pass that through log_softmax.\n",
    "        # Many non-linearities and other functions are in torch.nn.functional\n",
    "        return F.log_softmax(self.linear(bow_vector), dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 2\n",
    "vocab_size = len(inputsBOW.vocab)\n",
    "model = LogisticRegression(num_labels, vocab_size)\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch #0 [....................]\n",
      "epoch #1 [....................]\n",
      "epoch #2 [....................]\n",
      "epoch #3 [....................]\n",
      "epoch #4 [....................]\n",
      "epoch #5 [....................]\n",
      "epoch #6 [....................]\n",
      "epoch #7 [....................]\n",
      "epoch #8 [....................]\n",
      "epoch #9 [....................]\n",
      "epoch #10 [....................]\n",
      "epoch #11 [....................]\n",
      "epoch #12 [....................]\n",
      "epoch #13 [....................]\n",
      "epoch #14 [....................]\n",
      "epoch #15 [....................]\n",
      "epoch #16 [....................]\n",
      "epoch #17 [....................]\n",
      "epoch #18 [....................]\n",
      "epoch #19 [....................]\n",
      "epoch #20 [....................]\n",
      "epoch #21 [....................]\n",
      "epoch #22 [....................]\n",
      "epoch #23 [....................]\n",
      "epoch #24 [....................]\n",
      "epoch #25 [....................]\n",
      "epoch #26 [....................]\n",
      "epoch #27 [....................]\n",
      "epoch #28 [....................]\n",
      "epoch #29 [....................]\n",
      "epoch #30 [....................]\n",
      "epoch #31 [....................]\n",
      "epoch #32 [....................]\n",
      "epoch #33 [....................]\n",
      "epoch #34 [....................]\n",
      "epoch #35 [....................]\n",
      "epoch #36 [....................]\n",
      "epoch #37 [....................]\n",
      "epoch #38 [....................]\n",
      "epoch #39 [....................]\n",
      "epoch #40 [....................]\n",
      "epoch #41 [....................]\n",
      "epoch #42 [....................]\n",
      "epoch #43 [....................]\n",
      "epoch #44 [....................]\n",
      "epoch #45 [....................]\n",
      "epoch #46 [....................]\n",
      "epoch #47 [....................]\n",
      "epoch #48 [....................]\n",
      "epoch #49 [....................]\n",
      "epoch #50 [....................]\n",
      "epoch #51 [....................]\n",
      "epoch #52 [....................]\n",
      "epoch #53 [....................]\n",
      "epoch #54 [....................]\n",
      "epoch #55 [....................]\n",
      "epoch #56 [....................]\n",
      "epoch #57 [....................]\n",
      "epoch #58 [....................]\n",
      "epoch #59 [....................]\n",
      "epoch #60 [....................]\n",
      "epoch #61 [....................]\n",
      "epoch #62 [....................]\n",
      "epoch #63 [....................]\n",
      "epoch #64 [....................]\n",
      "epoch #65 [....................]\n",
      "epoch #66 [....................]\n",
      "epoch #67 [....................]\n",
      "epoch #68 [....................]\n",
      "epoch #69 [....................]\n",
      "epoch #70 [....................]\n",
      "epoch #71 [....................]\n",
      "epoch #72 [....................]\n",
      "epoch #73 [....................]\n",
      "epoch #74 [....................]\n",
      "epoch #75 [....................]\n",
      "epoch #76 [....................]\n",
      "epoch #77 [....................]\n",
      "epoch #78 [....................]\n",
      "epoch #79 [....................]\n",
      "epoch #80 [....................]\n",
      "epoch #81 [....................]\n",
      "epoch #82 [....................]\n",
      "epoch #83 [....................]\n",
      "epoch #84 [....................]\n",
      "epoch #85 [....................]\n",
      "epoch #86 [....................]\n",
      "epoch #87 [....................]\n",
      "epoch #88 [....................]\n",
      "epoch #89 [....................]\n",
      "epoch #90 [....................]\n",
      "epoch #91 [....................]\n",
      "epoch #92 [....................]\n",
      "epoch #93 [....................]\n",
      "epoch #94 [....................]\n",
      "epoch #95 [....................]\n",
      "epoch #96 [....................]\n",
      "epoch #97 [....................]\n",
      "epoch #98 [....................]\n",
      "epoch #99 [....................]\n"
     ]
    }
   ],
   "source": [
    "train_iter, dev_iter, test_iter = data.BucketIterator.splits(\n",
    "        (trainBOW, devBOW, testBOW), repeat=False, batch_size=100, device=-1)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('epoch #%s [' % epoch,end='.')\n",
    "    i=0\n",
    "    \n",
    "    for _,batch in enumerate(train_iter):\n",
    "        # Clear gradient before each new instance\n",
    "        model.zero_grad()\n",
    "        log_probs = model(batch.text)\n",
    "\n",
    "        # Compute the loss and gradients and update the parameters by opt.step()\n",
    "        loss = loss_fn(log_probs, batch.label)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        i += 1\n",
    "        if i % 50 == 0:\n",
    "            print('.',end='')\n",
    "    print(']')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "dev_loss = 0\n",
    "for idx, dev_batch in enumerate(dev_iter):\n",
    "    labels = dev_batch.label\n",
    "    prediction = model(dev_batch.text)\n",
    "    correct += (torch.max(prediction, 1)[1].view(dev_batch.label.size()).data == dev_batch.label.data).sum()\n",
    "    dev_loss = loss_fn(prediction, dev_batch.label)\n",
    "dev_acc = 100. * correct / len(devBOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.3073394495\n"
     ]
    }
   ],
   "source": [
    "print(dev_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Step 5: Validate\n",
    "    logloss = 0\n",
    "    for _, batch in enumerate(dev_iter):\n",
    "        log_probs = model(batch.text.data)\n",
    "        logloss += loss_fn(log_probs,batch.label.data)\n",
    "    print(\"Test log loss: \" % )\n",
    "    \n",
    "    \n",
    "# Validate\n",
    "for _, batch in enumerate(dev_iter):\n",
    "    log_probs = model(batch.text)\n",
    "    loss = loss_fn()\n",
    "    print(log_probs)\n",
    "    \n",
    "# CODE FROM PAPER\n",
    "# calculate accuracy on validation set\n",
    "n_dev_correct, dev_loss = 0, 0\n",
    "for dev_batch_idx, dev_batch in enumerate(dev_iter):\n",
    "     answer = model(dev_batch)\n",
    "     n_dev_correct += (torch.max(answer, 1)[1].view(dev_batch.label.size()).data == dev_batch.label.data).sum()\n",
    "     dev_loss = criterion(answer, dev_batch.label)\n",
    "dev_acc = 100. * n_dev_correct / len(dev)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    0     0     0  ...      0     0     0\n",
       "[torch.LongTensor of size 1x18844]"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.text.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
