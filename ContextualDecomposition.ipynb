{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\"\"\"\n",
    "Recall:\n",
    "h_t = beta_t + gamma_t\n",
    "c_t = beta_ct + gamma_ct\n",
    "\n",
    "p = SoftMax(W_betaT +W_gammaT)\n",
    "\n",
    "\n",
    "o_t = sigmoid( W_o*x_t + V_o*h_{t-1} + b_o )\n",
    "f_t = sigmoid( W_f*x_t + V_f*h_{t-1} + b_f ) \n",
    "i_t = sigmoid( W_i*x_t + V_i*h_{t-1} + b_i) \n",
    "g_t = tanh(W_g*x_t + V_g*h_{t-1} + b_g)\n",
    "c_t = f_t * c_{t-1} + i_t * g_t\n",
    "h_t = o_t * tanh(c_t)\n",
    "\"\"\"\n",
    "\n",
    "def linearize(a, b, c, activation_fn):\n",
    "    a_contrib = 0.5 * (activation_fn(a + b + c)  + activation_fn(a + c) - activation_fn(b + c) - activation_fn(c))\n",
    "    b_contrib = 0.5 * (activation_fn(a + b + c)  - activation_fn(a + c) + activation_fn(b + c) - activation_fn(c))\n",
    "    return a_contrib, b_contrib, activation_fn(c)\n",
    "\n",
    "def linearize_tanh(a, b):\n",
    "    a_contrib = 0.5 * (np.tanh(a) + np.tanh(a + b) - np.tanh(b)) \n",
    "    b_contrib = 0.5 * (np.tanh(b) + np.tanh(a + b) - np.tanh(a))\n",
    "    return a_contrib, b_contrib\n",
    "\n",
    "def CD(batch, model, start, stop):\n",
    "    weights = model.lstm.state_dict()\n",
    "    \n",
    "    W_ii, W_if, W_ig, W_io = np.split(weights['weight_ih_l0'], 4, 0)\n",
    "    W_hi, W_hf, W_hg, W_ho = np.split(weights['weight_hh_l0'], 4, 0)\n",
    "    W_out = model.hidden_to_label.weight.data\n",
    "    b_i, b_f, b_g, b_o = np.split(weights['bias_ih_l0'].cpu().numpy() + weights['bias_hh_l0'].cpu().numpy(), 4)\n",
    "    word_vecs = model.embed(batch.text)[:,0].data\n",
    "    \n",
    "    L = word_vecs.size(0)\n",
    "    phrase = np.zeros((L, model.hidden_dim))  #phrase contribution\n",
    "    rest = np.zeros((L, model.hidden_dim))    #rest of the contribution\n",
    "    phrase_h = np.zeros((L, model.hidden_dim))\n",
    "    rest_h = np.zeros((L, model.hidden_dim))\n",
    "    \n",
    "    #iterate through word_vecs\n",
    "    for i in range(L):\n",
    "        if i == 0:\n",
    "            #there is no prev\n",
    "            phrase_h_prev = np.zeros(model.hidden_dim)\n",
    "            rest_h_prev = np.zeros(model.hidden_dim)\n",
    "        else:\n",
    "            phrase_h_prev = phrase_h[i-1]\n",
    "            rest_h_prev = rest_h[i-1]\n",
    "            \n",
    "        #calculating o, f, i, g    \n",
    "        phrase_o = np.dot(W_ho, prev_phrase_h)\n",
    "        phrase_f = np.dot(W_hf, prev_phrase_h)\n",
    "        phrase_i = np.dot(W_hi, prev_phrase_h)\n",
    "        phrase_g = np.dot(W_hg, prev_phrase_h)\n",
    "        \n",
    "        rest_o = np.dot(W_ho, prev_rest_h)\n",
    "        rest_f = np.dot(W_hf, prev_rest_h)\n",
    "        rest_i = np.dot(W_hi, prev_rest_h)\n",
    "        rest_g = np.dot(W_hg, prev_rest_h)\n",
    "\n",
    "        #only modify for range [start, stop]\n",
    "        if (start <= i) and (i <= stop):\n",
    "            phrase_o = phrase_o + np.dot(W_io, word_vecs[i])\n",
    "            phrase_f = phrase_f + np.dot(W_if, word_vecs[i])\n",
    "            phrase_i = phrase_i + np.dot(W_ii, word_vecs[i])\n",
    "            phrase_g = phrase_g + np.dot(W_ig, word_vecs[i])\n",
    "        else:\n",
    "            rest_o = rest_o + np.dot(W_io, word_vecs[i])\n",
    "            rest_f = rest_f + np.dot(W_if, word_vecs[i])\n",
    "            rest_i = rest_i + np.dot(W_ii, word_vecs[i])\n",
    "            rest_g = rest_g + np.dot(W_ig, word_vecs[i])\n",
    "        \n",
    "        #calculate contributions to i, g\n",
    "        phrase_contrib_i, rest_contrib_i, bias_contrib_i = linearize(phrase_i, rest_i, b_i, sigmoid)\n",
    "        phrase_contrib_g, rest_contrib_g, bias_contrib_g = linearize(phrase_g, rest_g, b_g, np.tanh)\n",
    "\n",
    "        phrase[i] = phrase_contrib_i * (phrase_contrib_g + bias_contrib_g) + bias_contrib_i * phrase_contrib_g\n",
    "        rest[i] = rest_contrib_i * (phrase_contrib_g + rest_contrib_g + bias_contrib_grest) + (phrase_contrib_i + bias_contrib_i) * rest_contrib_g\n",
    "\n",
    "        #add bias for range [start,stop)\n",
    "        if i >= start and i < stop:\n",
    "            phrase[i] += bias_contrib_i * bias_contrib_g\n",
    "        else:\n",
    "            rest[i] += bias_contrib_i * bias_contrib_g\n",
    "        \n",
    "        #When there's a prev, calculate contributions\n",
    "        if i > 0:\n",
    "            phrase_contrib_f, rest_contrib_f, bias_contrib_f = linearize(phrase_f, rest_f, b_f, sigmoid)\n",
    "            phrase[i] += (phrase_contrib_f + bias_contrib_f) * phrase[i-1]\n",
    "            rest[i] += (phrase_contrib_f + rest_contrib_f + bias_contrib_f) * rest[i-1] + rest_contrib_f * phrase[i-1]\n",
    "\n",
    "        o = sigmoid(np.dot(W_io, word_vecs[i]) + np.dot(W_ho, prev_phrase_h + prev_rest_h) + b_o)\n",
    "        phrase_contrib_o, rest_contrib_o, bias_contrib_o = linearize(phrase_o, rest_o, b_o, sigmoid)\n",
    "        new_phrase_h, new_rest_h = linearize_tanh(phrase[i], rest[i])\n",
    "        phrase_h[i] = o * new_phrase_h\n",
    "        rest_h[i] = o * new_rest_h\n",
    "    \n",
    "    #calculating final scores\n",
    "    phrase_scores = np.dot(W_out, phrase_h[L-1])\n",
    "    rest_scores = np.dot(W_out, rest_h[L-1])\n",
    "\n",
    "    return phrase_scores, rest_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
